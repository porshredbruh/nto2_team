"""
Enhanced inference script with ensemble predictions.
"""

import numpy as np
import pandas as pd
import lightgbm as lgb
import joblib
import json
import warnings
warnings.filterwarnings('ignore')

from . import config, constants
from .data_processing import expand_candidates, load_and_merge_data
from .features import add_aggregate_features, handle_missing_values


def predict_with_ensemble() -> None:
    """Generates ensemble predictions for the test set."""
    
    # Load targets and candidates
    print("Loading targets and candidates...")
    targets_df = pd.read_csv(
        config.RAW_DATA_DIR / constants.TARGETS_FILENAME,
        dtype={constants.COL_USER_ID: "int32"},
    )
    candidates_df = pd.read_csv(
        config.RAW_DATA_DIR / constants.CANDIDATES_FILENAME,
        dtype={constants.COL_USER_ID: "int32"},
    )

    print(f"Targets: {len(targets_df):,} users")
    print(f"Candidates: {len(candidates_df):,} users")

    # Expand candidates into pairs
    print("\nExpanding candidates...")
    candidates_pairs_df = expand_candidates(candidates_df)

    # Load prepared data for base features
    processed_path = config.PROCESSED_DATA_DIR / constants.PROCESSED_DATA_FILENAME
    if not processed_path.exists():
        raise FileNotFoundError(
            f"Processed data not found at {processed_path}. "
            "Please run 'poetry run python -m src.baseline.prepare_data' first."
        )

    print(f"Loading prepared data from {processed_path}...")
    featured_df = pd.read_parquet(processed_path, engine="pyarrow")

    # Get train data for computing aggregates
    train_df = featured_df[featured_df[constants.COL_SOURCE] == constants.VAL_SOURCE_TRAIN].copy()

    # Load metadata for candidates
    print("Loading metadata...")
    _, _, _, book_genres_df, descriptions_df = load_and_merge_data()
    user_data_df = pd.read_csv(config.RAW_DATA_DIR / constants.USER_DATA_FILENAME)
    book_data_df = pd.read_csv(config.RAW_DATA_DIR / constants.BOOK_DATA_FILENAME)

    # Merge metadata with candidates
    print("Merging metadata with candidates...")
    candidates_with_meta = candidates_pairs_df.merge(user_data_df, on=constants.COL_USER_ID, how="left")
    book_data_df = book_data_df.drop_duplicates(subset=[constants.COL_BOOK_ID])
    candidates_with_meta = candidates_with_meta.merge(book_data_df, on=constants.COL_BOOK_ID, how="left")

    # Add base features from prepared data
    print("Adding base features...")
    
    # Identify feature columns from prepared data
    exclude_base_cols = [
        constants.COL_USER_ID,
        constants.COL_BOOK_ID,
        constants.COL_SOURCE,
        constants.COL_TIMESTAMP,
        constants.COL_HAS_READ,
        config.TARGET,
        constants.COL_PREDICTION,
        constants.COL_GENDER,
        constants.COL_AGE,
        constants.COL_AUTHOR_ID,
        constants.COL_PUBLICATION_YEAR,
        constants.COL_LANGUAGE,
        constants.COL_PUBLISHER,
        constants.COL_AVG_RATING,
    ]
    
    feature_cols = [col for col in featured_df.columns if col not in exclude_base_cols]
    
    # Get a representative row for each book
    book_features_df = featured_df[[constants.COL_BOOK_ID] + feature_cols].drop_duplicates(
        subset=[constants.COL_BOOK_ID]
    )

    # Merge book features
    cols_to_drop = [col for col in feature_cols if col in candidates_with_meta.columns]
    if cols_to_drop:
        candidates_with_meta = candidates_with_meta.drop(columns=cols_to_drop)

    candidates_with_meta = candidates_with_meta.merge(
        book_features_df, on=constants.COL_BOOK_ID, how="left"
    )

    # Add text features
    print("Adding text features...")
    tfidf_cols = [col for col in featured_df.columns if col.startswith("tfidf_")]
    bert_cols = [col for col in featured_df.columns if col.startswith("bert")]
    text_feature_cols = tfidf_cols + bert_cols

    if text_feature_cols:
        book_text_features = featured_df[[constants.COL_BOOK_ID] + text_feature_cols].drop_duplicates(
            subset=[constants.COL_BOOK_ID]
        )
        candidates_with_meta = candidates_with_meta.merge(
            book_text_features, on=constants.COL_BOOK_ID, how="left"
        )

    # Compute aggregate features on ALL train data
    print("\nComputing aggregate features on all train data...")
    candidates_with_agg = add_aggregate_features(candidates_with_meta.copy(), train_df)

    # Handle missing values
    print("Handling missing values...")
    candidates_final = handle_missing_values(candidates_with_agg, train_df)

    # Load feature list saved during training
    features_path = config.MODEL_DIR / "features_list.json"
    if features_path.exists():
        print("Loading feature list from training...")
        with open(features_path, "r") as f:
            features = json.load(f)
        print(f"Loaded {len(features)} features from training")
    else:
        print("Warning: Feature list not found, using all available features")
        exclude_cols = exclude_base_cols + [
            constants.COL_USER_ID,
            constants.COL_BOOK_ID,
        ]
        features = [col for col in candidates_final.columns if col not in exclude_cols]

    # Ensure all features exist
    missing_features = [f for f in features if f not in candidates_final.columns]
    if missing_features:
        print(f"Warning: Missing {len(missing_features)} features, adding defaults")
        for feat in missing_features:
            candidates_final[feat] = 0.0

    # Keep only necessary columns
    keep_cols = features + [constants.COL_USER_ID, constants.COL_BOOK_ID]
    candidates_final = candidates_final[keep_cols]

    # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–∞—è —á–∞—Å—Ç—å: –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print("\nüéØ Critical step: Preparing categorical features for LightGBM...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    train_categorical_info = {}
    for col in features:
        if col in train_df.columns and train_df[col].dtype.name == "category":
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            train_categories = list(train_df[col].cat.categories)
            train_categorical_info[col] = train_categories
            print(f"   Found categorical feature: {col} with {len(train_categories)} categories")
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    for col in features:
        if col in train_categorical_info:
            # –≠—Ç–æ—Ç –ø—Ä–∏–∑–Ω–∞–∫ –±—ã–ª –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
            train_categories = train_categorical_info[col]
            
            if col not in candidates_final.columns:
                print(f"   ‚ö†Ô∏è  Categorical feature {col} not in test data, adding default")
                candidates_final[col] = train_categories[0] if train_categories else "missing"
            else:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫–∏ –∏ –∑–∞–º–µ–Ω—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –æ–±—É—á–∞—é—â–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö
                candidates_final[col] = candidates_final[col].astype(str)
                
                # –ù–∞—Ö–æ–¥–∏–º –∑–Ω–∞—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –æ–±—É—á–∞—é—â–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö
                unique_vals = set(candidates_final[col].unique())
                train_vals_set = set(train_categories)
                unknown_vals = unique_vals - train_vals_set
                
                if unknown_vals:
                    print(f"   ‚ö†Ô∏è  Feature {col} has {len(unknown_vals)} unknown values, replacing with first category")
                    # –ó–∞–º–µ–Ω—è–µ–º –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–µ—Ä–≤–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π
                    candidates_final.loc[candidates_final[col].isin(unknown_vals), col] = train_categories[0] if train_categories else "missing"
                
                # –°–æ–∑–¥–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π —Ç–∏–ø —Å —Ç–µ–º–∏ –∂–µ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏, —á—Ç–æ –∏ –≤ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
                candidates_final[col] = pd.Categorical(
                    candidates_final[col],
                    categories=train_categories,
                    ordered=False
                )
        else:
            # –ü—Ä–∏–∑–Ω–∞–∫ –Ω–µ –±—ã–ª –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
            if col in candidates_final.columns:
                if candidates_final[col].dtype.name == "category":
                    # –ï—Å–ª–∏ –≤ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ–Ω –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π, –Ω–æ –Ω–µ –±—ã–ª –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ - –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É
                    candidates_final[col] = candidates_final[col].astype(str)
                elif candidates_final[col].dtype.name == "object":
                    # Object —Ç–∏–ø —Ç–∞–∫–∂–µ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É
                    candidates_final[col] = candidates_final[col].astype(str)
    
    print(f"‚úÖ Categorical features prepared for LightGBM")

    X_test = candidates_final[features]
    print(f"Prediction features: {len(features)}")
    print(f"Test data shape: {X_test.shape}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º
    print("\nüîç Checking data types before prediction...")
    dtypes_summary = X_test.dtypes.value_counts()
    for dtype, count in dtypes_summary.items():
        print(f"   {dtype}: {count} columns")
    
    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –Ω–µ—Ç datetime —Ç–∏–ø–æ–≤
    datetime_cols = X_test.select_dtypes(include=['datetime64', 'timedelta64']).columns
    if len(datetime_cols) > 0:
        print(f"   ‚ö†Ô∏è  Found datetime columns: {list(datetime_cols)}")
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏—Ö –≤ —á–∏—Å–ª–æ–≤—ã–µ (timestamp)
        for col in datetime_cols:
            X_test[col] = X_test[col].astype(np.int64) // 10**9

    # Load models for ensemble
    print("\nü§ñ Loading ensemble models...")
    
    # Load LightGBM model
    lgb_path = config.MODEL_DIR / config.MODEL_FILENAME
    if not lgb_path.exists():
        raise FileNotFoundError(f"LightGBM model not found at {lgb_path}")
    
    print(f"Loading LightGBM model from {lgb_path}...")
    lgb_model = lgb.Booster(model_file=str(lgb_path))
    
    # Try to load CatBoost model
    cb_path = config.MODEL_DIR / config.CATBOOST_MODEL_FILENAME
    use_catboost = False
    cb_model = None
    
    if cb_path.exists():
        try:
            import catboost as cb
            print(f"Loading CatBoost model from {cb_path}...")
            cb_model = cb.CatBoostClassifier()
            cb_model.load_model(str(cb_path))
            use_catboost = True
            print("‚úÖ CatBoost model loaded")
        except Exception as e:
            print(f"‚ö†Ô∏è  Failed to load CatBoost model: {e}")
    else:
        print("‚ÑπÔ∏è  CatBoost model not found, using LightGBM only")

    # Generate predictions
    print("\nGenerating ensemble predictions...")
    
    # LightGBM predictions - –û–ß–ï–ù–¨ –í–ê–ñ–ù–û: –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
    print("Preparing data for LightGBM prediction...")
    
    # –î–ª—è LightGBM –Ω—É–∂–Ω–æ —è–≤–Ω–æ —É–∫–∞–∑–∞—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    categorical_indices = []
    for i, col in enumerate(features):
        if col in train_categorical_info:
            categorical_indices.append(i)
    
    print(f"   LightGBM will use {len(categorical_indices)} categorical features")
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º DataFrame –≤ numpy –º–∞—Å—Å–∏–≤ –¥–ª—è LightGBM
    # LightGBM –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å pandas DataFrame, –Ω–æ –ª—É—á—à–µ —è–≤–Ω–æ —É–∫–∞–∑–∞—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    X_test_for_lgb = X_test.copy()
    
    # –î–ª—è LightGBM –Ω—É–∂–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–µ –∫–æ–¥—ã
    for col_idx, col in enumerate(features):
        if col in train_categorical_info:
            # –£–∂–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π —Ç–∏–ø, LightGBM —Å–∞–º –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤ –∫–æ–¥—ã
            pass
    
    # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ LightGBM
    print("Making LightGBM predictions...")
    try:
        lgb_proba_all = lgb_model.predict(X_test_for_lgb)
        lgb_proba_all = np.array(lgb_proba_all)
        if lgb_proba_all.ndim == 1:
            lgb_proba_all = lgb_proba_all.reshape(-1, 3)
        print(f"   LightGBM predictions shape: {lgb_proba_all.shape}")
    except Exception as e:
        print(f"‚ùå LightGBM prediction failed: {e}")
        # –ü–æ–ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥
        print("   Trying alternative prediction method...")
        X_test_array = X_test_for_lgb.values.astype(np.float32)
        lgb_proba_all = lgb_model.predict(X_test_array)
        lgb_proba_all = np.array(lgb_proba_all)
        if lgb_proba_all.ndim == 1:
            lgb_proba_all = lgb_proba_all.reshape(-1, 3)
        print(f"   LightGBM predictions shape: {lgb_proba_all.shape}")
    
    if use_catboost and cb_model is not None:
        # CatBoost predictions
        print("Generating CatBoost predictions...")
        
        # Prepare data for CatBoost
        # CatBoost —Ç—Ä–µ–±—É–µ—Ç –æ—Å–æ–±–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X_test_cb = X_test.copy()
        categorical_features_for_cb = [col for col in features if col in train_categorical_info]
        categorical_indices_cb = [features.index(f) for f in categorical_features_for_cb if f in features]
        
        # CatBoost —Ç—Ä–µ–±—É–µ—Ç, —á—Ç–æ–±—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –±—ã–ª–∏ —Å—Ç—Ä–æ–∫–æ–≤—ã–º–∏
        for col in categorical_features_for_cb:
            if col in X_test_cb.columns:
                X_test_cb[col] = X_test_cb[col].astype(str).fillna("missing")
        
        try:
            cb_proba_all = cb_model.predict_proba(X_test_cb)
            print(f"   CatBoost predictions shape: {cb_proba_all.shape}")
            
            # Ensemble weights (–º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å)
            lgb_weight = 0.6
            cb_weight = 0.4
            
            ensemble_proba = lgb_weight * lgb_proba_all + cb_weight * cb_proba_all
            print(f"   Ensemble weights: LightGBM={lgb_weight}, CatBoost={cb_weight}")
        except Exception as e:
            print(f"‚ö†Ô∏è  CatBoost prediction failed: {e}")
            print("   Using LightGBM predictions only")
            ensemble_proba = lgb_proba_all
    else:
        ensemble_proba = lgb_proba_all
        print("Using LightGBM predictions only")
    
    # Enhanced ranking score calculation
    print("Calculating enhanced ranking scores...")
    
    # –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
    class_weights = np.array([0.0, 1.0, 3.0])  # cold: 0, planned: 1, read: 3
    
    # –ë–∞–∑–æ–≤—ã–π —Å–∫–æ—Ä
    ranking_scores = np.sum(ensemble_proba * class_weights, axis=1)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ø—Ä–∞–≤–∫—É –Ω–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
    confidence = np.max(ensemble_proba, axis=1)
    ranking_scores = ranking_scores * (0.3 + 0.7 * confidence)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ø—Ä–∞–≤–∫—É –Ω–∞ —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏ 2 –∏ 1
    proba_diff = ensemble_proba[:, 2] - ensemble_proba[:, 1]
    ranking_scores = ranking_scores * (1.0 + 0.5 * np.tanh(proba_diff))
    
    candidates_final["prediction"] = ranking_scores

    # Rank candidates for each user and select top-K
    print("\nRanking candidates for each user...")
    submission_rows = []

    for user_id in targets_df[constants.COL_USER_ID]:
        user_candidates = candidates_final[candidates_final[constants.COL_USER_ID] == user_id].copy()

        if len(user_candidates) == 0:
            # No candidates for this user - empty list
            book_id_list = ""
        else:
            # Sort by prediction probability (descending)
            user_candidates = user_candidates.sort_values("prediction", ascending=False)

            # Select top-K, where K = min(20, num_candidates)
            k = min(constants.MAX_RANKING_LENGTH, len(user_candidates))
            top_books = user_candidates.head(k)

            # Create comma-separated string of book_ids
            book_id_list = ",".join([str(int(book_id)) for book_id in top_books[constants.COL_BOOK_ID]])

        submission_rows.append({constants.COL_USER_ID: user_id, constants.COL_BOOK_ID_LIST: book_id_list})

    # Create submission DataFrame
    submission_df = pd.DataFrame(submission_rows)

    # Ensure submission directory exists
    config.SUBMISSION_DIR.mkdir(parents=True, exist_ok=True)
    submission_path = config.SUBMISSION_DIR / constants.SUBMISSION_FILENAME

    # Save submission
    submission_df.to_csv(submission_path, index=False)
    print(f"\n‚úÖ Submission file created at: {submission_path}")
    print(f"   Submission shape: {submission_df.shape}")

    # Print statistics
    non_empty = submission_df[submission_df[constants.COL_BOOK_ID_LIST] != ""].shape[0]
    avg_books = submission_df[constants.COL_BOOK_ID_LIST].apply(
        lambda x: len(x.split(",")) if x else 0
    ).mean()
    
    print(f"üìä Submission statistics:")
    print(f"   Users with recommendations: {non_empty}/{len(submission_df)}")
    print(f"   Average books per user: {avg_books:.2f}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    predictions_path = config.SUBMISSION_DIR / "predictions_with_scores.parquet"
    candidates_final[["user_id", "book_id", "prediction"]].to_parquet(predictions_path)
    print(f"   Detailed predictions saved to: {predictions_path}")


if __name__ == "__main__":
    predict_with_ensemble()