{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "НАЧАЛО РАБОТЫ ПАЙПЛАЙНА\n",
      "============================================================\n",
      "\n",
      "ЗАГРУЗКА ДАННЫХ\n",
      "[100]\tvalid_0's multi_logloss: 0.442086\tvalid_0's multi_error: 0.19277\n",
      "[200]\tvalid_0's multi_logloss: 0.385062\tvalid_0's multi_error: 0.192585\n",
      "[100]\tvalid_0's multi_logloss: 0.437953\tvalid_0's multi_error: 0.188724\n",
      "[200]\tvalid_0's multi_logloss: 0.379992\tvalid_0's multi_error: 0.188583\n",
      "[100]\tvalid_0's multi_logloss: 0.437546\tvalid_0's multi_error: 0.18916\n",
      "[200]\tvalid_0's multi_logloss: 0.380143\tvalid_0's multi_error: 0.18878\n",
      "\n",
      "РЕЗУЛЬТАТЫ:\n",
      "Средняя точность: 0.8103\n",
      "Средний NDCG@20: 0.9300\n",
      "Submission сохранен: output/submissions/submission_gpu_enhanced_20251209_075527.csv\n",
      "\n",
      "Готово!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import ndcg_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import gc\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"НАЧАЛО РАБОТЫ ПАЙПЛАЙНА\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "DATA_DIR = Path(\"data/raw\")\n",
    "MODEL_DIR = Path(\"output/models\")\n",
    "SUBMISSION_DIR = Path(\"output/submissions\")\n",
    "\n",
    "for dir_path in [DATA_DIR, MODEL_DIR, SUBMISSION_DIR]:\n",
    "    dir_path.mkdir(exist_ok=True)\n",
    "\n",
    "def safe_load_csv(filepath):\n",
    "    try:\n",
    "        return pd.read_csv(filepath, low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка загрузки {filepath}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "print(\"\\nЗАГРУЗКА ДАННЫХ\")\n",
    "train = safe_load_csv(DATA_DIR / \"train.csv\")\n",
    "books = safe_load_csv(DATA_DIR / \"books.csv\")\n",
    "users = safe_load_csv(DATA_DIR / \"users.csv\")\n",
    "candidates = safe_load_csv(DATA_DIR / \"candidates.csv\")\n",
    "targets = safe_load_csv(DATA_DIR / \"targets.csv\")\n",
    "\n",
    "try:\n",
    "    genres = safe_load_csv(DATA_DIR / \"genres.csv\")\n",
    "    book_genres = safe_load_csv(DATA_DIR / \"book_genres.csv\")\n",
    "    book_descriptions = safe_load_csv(DATA_DIR / \"book_descriptions.csv\")\n",
    "except:\n",
    "    genres, book_genres, book_descriptions = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "if 'age' in users.columns:\n",
    "    users['age'] = pd.to_numeric(users['age'], errors='coerce')\n",
    "    users['age'] = users['age'].fillna(users['age'].median())\n",
    "    age_bins = [0, 18, 25, 35, 50, 100]\n",
    "    age_labels = ['teen', 'young', 'adult', 'middle', 'senior']\n",
    "    users['age_group'] = pd.cut(users['age'], bins=age_bins, labels=age_labels, include_lowest=True).astype(str)\n",
    "    users['age_group'] = users['age_group'].fillna('adult')\n",
    "\n",
    "if 'gender' in users.columns:\n",
    "    users['gender'] = users['gender'].fillna('unknown')\n",
    "    gender_encoder = LabelEncoder()\n",
    "    users['gender_encoded'] = gender_encoder.fit_transform(users['gender'])\n",
    "\n",
    "if 'description' in books.columns:\n",
    "    books['desc_len'] = books['description'].str.len().fillna(0)\n",
    "    books['has_desc'] = books['description'].notna().astype('int8')\n",
    "    books['desc_word_count'] = books['description'].str.split().str.len().fillna(0)\n",
    "\n",
    "if 'publication_year' in books.columns:\n",
    "    books['publication_year'] = pd.to_numeric(books['publication_year'], errors='coerce')\n",
    "    books['publication_year'] = books['publication_year'].fillna(books['publication_year'].median())\n",
    "    books['book_age'] = 2025 - books['publication_year']\n",
    "    books['is_old_book'] = (books['book_age'] > 20).astype(int)\n",
    "    books['is_recent_book'] = (books['book_age'] <= 5).astype(int)\n",
    "\n",
    "if 'avg_rating' in books.columns:\n",
    "    books['avg_rating'] = pd.to_numeric(books['avg_rating'], errors='coerce')\n",
    "    books['avg_rating'] = books['avg_rating'].fillna(books['avg_rating'].median())\n",
    "    books['high_rated'] = (books['avg_rating'] >= 8).astype(int)\n",
    "    books['low_rated'] = (books['avg_rating'] <= 4).astype(int)\n",
    "\n",
    "if not book_genres.empty and not genres.empty:\n",
    "    train_with_genres = train.merge(book_genres, on='book_id', how='left')\n",
    "    genre_popularity = train_with_genres.groupby('genre_id').agg({\n",
    "        'has_read': ['sum', 'count', 'mean']\n",
    "    }).reset_index()\n",
    "    genre_popularity.columns = ['genre_id', 'genre_read_sum', 'genre_interactions', 'genre_read_rate']\n",
    "    book_genres_extended = book_genres.merge(genre_popularity, on='genre_id', how='left')\n",
    "    book_genre_stats = book_genres_extended.groupby('book_id').agg({\n",
    "        'genre_id': 'nunique',\n",
    "        'genre_read_rate': ['mean', 'max', 'min', 'std']\n",
    "    }).reset_index()\n",
    "    book_genre_stats.columns = ['book_id', 'num_genres', 'avg_genre_read_rate', 'max_genre_read_rate', \n",
    "                               'min_genre_read_rate', 'std_genre_read_rate']\n",
    "    books = books.merge(book_genre_stats, on='book_id', how='left')\n",
    "    for col in ['num_genres', 'avg_genre_read_rate', 'max_genre_read_rate', 'min_genre_read_rate', 'std_genre_read_rate']:\n",
    "        if col in books.columns:\n",
    "            books[col] = books[col].fillna(0)\n",
    "\n",
    "user_stats = train.groupby('user_id').agg({\n",
    "    'has_read': ['sum', 'count', 'mean'],\n",
    "    'book_id': 'nunique'\n",
    "}).reset_index()\n",
    "user_stats.columns = ['user_id', 'read_sum', 'total_interactions', 'read_rate', 'unique_books']\n",
    "user_stats['plan_rate'] = 1 - user_stats['read_rate']\n",
    "user_stats['diversity'] = user_stats['unique_books'] / user_stats['total_interactions'].replace(0, 1)\n",
    "user_stats['engagement'] = user_stats['total_interactions'] / user_stats['unique_books'].replace(0, 1)\n",
    "user_stats['conversion_rate'] = user_stats['read_sum'] / user_stats['total_interactions'].replace(0, 1)\n",
    "\n",
    "if 'timestamp' in train.columns:\n",
    "    train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "    user_time_stats = train.groupby('user_id').agg({\n",
    "        'timestamp': ['min', 'max', 'nunique']\n",
    "    }).reset_index()\n",
    "    user_time_stats.columns = ['user_id', 'first_interaction', 'last_interaction', 'unique_dates']\n",
    "    user_time_stats['activity_duration_days'] = (user_time_stats['last_interaction'] - user_time_stats['first_interaction']).dt.days\n",
    "    user_time_stats['activity_duration_days'] = user_time_stats['activity_duration_days'].clip(lower=1)\n",
    "    user_stats = user_stats.merge(user_time_stats, on='user_id', how='left')\n",
    "    user_stats['interactions_per_day'] = user_stats['total_interactions'] / user_stats['activity_duration_days'].replace(0, 1)\n",
    "    user_stats['reading_frequency'] = user_stats['read_sum'] / user_stats['activity_duration_days'].replace(0, 1)\n",
    "\n",
    "book_stats = train.groupby('book_id').agg({\n",
    "    'has_read': ['sum', 'count', 'mean'],\n",
    "    'user_id': 'nunique'\n",
    "}).reset_index()\n",
    "book_stats.columns = ['book_id', 'book_read_sum', 'book_interactions', 'book_read_rate', 'unique_users']\n",
    "book_stats['book_plan_rate'] = 1 - book_stats['book_read_rate']\n",
    "book_stats['book_diversity'] = book_stats['unique_users'] / book_stats['book_interactions'].replace(0, 1)\n",
    "book_stats['book_popularity'] = book_stats['book_interactions'] / len(user_stats)\n",
    "book_stats['book_conversion_rate'] = book_stats['book_read_sum'] / book_stats['book_interactions'].replace(0, 1)\n",
    "book_stats['book_popularity_norm'] = (book_stats['book_interactions'] - book_stats['book_interactions'].min()) / (book_stats['book_interactions'].max() - book_stats['book_interactions'].min() + 1e-10)\n",
    "book_stats['book_read_rate_norm'] = (book_stats['book_read_rate'] - book_stats['book_read_rate'].min()) / (book_stats['book_read_rate'].max() - book_stats['book_read_rate'].min() + 1e-10)\n",
    "\n",
    "user_features = users.merge(user_stats, on='user_id', how='left')\n",
    "user_feature_cols = ['read_sum', 'total_interactions', 'read_rate', 'unique_books', 'plan_rate', 'diversity', 'engagement', 'conversion_rate']\n",
    "if 'interactions_per_day' in user_features.columns:\n",
    "    user_feature_cols.extend(['interactions_per_day', 'reading_frequency', 'activity_duration_days'])\n",
    "for col in user_feature_cols:\n",
    "    if col in user_features.columns:\n",
    "        user_features[col] = user_features[col].fillna(0)\n",
    "\n",
    "book_features = books.merge(book_stats, on='book_id', how='left')\n",
    "book_feature_cols = ['book_read_sum', 'book_interactions', 'book_read_rate', 'unique_users', 'book_plan_rate', 'book_diversity', 'book_popularity', 'book_conversion_rate', 'book_popularity_norm', 'book_read_rate_norm']\n",
    "if 'num_genres' in book_features.columns:\n",
    "    book_feature_cols.extend(['num_genres', 'avg_genre_read_rate', 'max_genre_read_rate', 'min_genre_read_rate', 'std_genre_read_rate'])\n",
    "for col in book_feature_cols:\n",
    "    if col in book_features.columns:\n",
    "        book_features[col] = book_features[col].fillna(0)\n",
    "\n",
    "train_pairs_set = set(zip(train['user_id'], train['book_id']))\n",
    "np.random.seed(42)\n",
    "negative_samples = []\n",
    "top_books = train['book_id'].value_counts().head(200).index.tolist()\n",
    "all_users = user_stats['user_id'].unique()\n",
    "n_users_to_sample = min(3000, len(all_users))\n",
    "\n",
    "for user_id in all_users[:n_users_to_sample]:\n",
    "    user_books = set(train[train['user_id'] == user_id]['book_id'].unique())\n",
    "    candidate_books = [b for b in top_books if b not in user_books]\n",
    "    if len(candidate_books) >= 2:\n",
    "        selected_books = np.random.choice(candidate_books, min(2, len(candidate_books)), replace=False)\n",
    "        for book_id in selected_books:\n",
    "            negative_samples.append({'user_id': user_id, 'book_id': book_id, 'has_read': -1})\n",
    "\n",
    "for user_id in np.random.choice(all_users, min(1500, len(all_users)), replace=False):\n",
    "    user_row = user_features[user_features['user_id'] == user_id]\n",
    "    if len(user_row) > 0:\n",
    "        user_read_rate = user_row['read_rate'].values[0]\n",
    "        user_diversity = user_row['diversity'].values[0]\n",
    "        temp_books = book_features.copy()\n",
    "        temp_books['compatibility'] = (user_read_rate * temp_books['book_read_rate'] + user_diversity * temp_books['book_diversity'])\n",
    "        user_books = set(train[train['user_id'] == user_id]['book_id'].unique())\n",
    "        low_compat_books = temp_books[~temp_books['book_id'].isin(user_books)]\n",
    "        low_compat_books = low_compat_books.nsmallest(10, 'compatibility')['book_id'].tolist()\n",
    "        if len(low_compat_books) >= 1:\n",
    "            selected_books = np.random.choice(low_compat_books, min(1, len(low_compat_books)), replace=False)\n",
    "            for book_id in selected_books:\n",
    "                negative_samples.append({'user_id': user_id, 'book_id': book_id, 'has_read': -1})\n",
    "\n",
    "negative_df = pd.DataFrame(negative_samples)\n",
    "train_extended = pd.concat([train, negative_df], ignore_index=True)\n",
    "del negative_df, negative_samples\n",
    "gc.collect()\n",
    "\n",
    "train_features = train_extended.merge(user_features, on='user_id', how='left')\n",
    "train_features = train_features.merge(book_features, on='book_id', how='left')\n",
    "train_features['read_rate_diff'] = train_features['read_rate'] - train_features['book_read_rate']\n",
    "train_features['interaction_ratio'] = train_features['total_interactions'] / (train_features['book_interactions'] + 1)\n",
    "train_features['compatibility_score'] = (train_features['read_rate'] * train_features['book_read_rate'] + train_features['diversity'] * train_features['book_diversity'])\n",
    "train_features['user_book_engagement_ratio'] = train_features['engagement'] / (train_features['book_popularity'] + 1e-10)\n",
    "train_features['user_book_diversity_product'] = train_features['diversity'] * train_features['book_diversity']\n",
    "train_features['has_interaction'] = train_features.apply(lambda row: 1 if (row['user_id'], row['book_id']) in train_pairs_set else 0, axis=1)\n",
    "train_features['is_likely_read'] = ((train_features['read_rate'] > 0.5) & (train_features['book_read_rate'] > 0.3)).astype(int)\n",
    "train_features['is_likely_plan'] = ((train_features['plan_rate'] > 0.5) & (train_features['book_read_rate'] < 0.3)).astype(int)\n",
    "train_features['target'] = train_features['has_read'].map({1: 2, 0: 1, -1: 0})\n",
    "\n",
    "feature_columns = [\n",
    "    'read_rate', 'total_interactions', 'unique_books', 'plan_rate', 'diversity', 'engagement', 'conversion_rate',\n",
    "    'age', 'gender_encoded',\n",
    "    'book_read_rate', 'book_interactions', 'unique_users', 'book_plan_rate', 'book_diversity', 'book_popularity', 'book_conversion_rate',\n",
    "    'desc_len', 'has_desc', 'desc_word_count', 'avg_rating', 'high_rated', 'low_rated', 'book_age', 'is_old_book', 'is_recent_book',\n",
    "    'book_popularity_norm', 'book_read_rate_norm',\n",
    "    'read_rate_diff', 'interaction_ratio', 'compatibility_score', 'user_book_engagement_ratio', 'user_book_diversity_product',\n",
    "    'num_genres', 'avg_genre_read_rate', 'max_genre_read_rate', 'min_genre_read_rate', 'std_genre_read_rate',\n",
    "    'interactions_per_day', 'reading_frequency', 'activity_duration_days',\n",
    "    'has_interaction', 'is_likely_read', 'is_likely_plan'\n",
    "]\n",
    "\n",
    "available_features = [f for f in feature_columns if f in train_features.columns]\n",
    "X = train_features[available_features].copy()\n",
    "y = train_features['target'].copy()\n",
    "del train_extended\n",
    "gc.collect()\n",
    "\n",
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 3,\n",
    "    'metric': ['multi_logloss', 'multi_error'],\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 127,\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 20,\n",
    "    'min_split_gain': 0.01,\n",
    "    'reg_alpha': 0.3,\n",
    "    'reg_lambda': 0.3,\n",
    "    'verbose': -1,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'device': 'gpu',\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0,\n",
    "    'max_bin': 63,\n",
    "    'tree_learner': 'data',\n",
    "    'is_unbalance': True,\n",
    "    'scale_pos_weight': 1.2,\n",
    "}\n",
    "\n",
    "groups = train_features['user_id']\n",
    "group_kfold = GroupKFold(n_splits=3)\n",
    "cv_models = []\n",
    "cv_scores = []\n",
    "cv_accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(group_kfold.split(X, y, groups)):\n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    train_data = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "    val_data = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_data)\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        valid_sets=[val_data],\n",
    "        num_boost_round=2000,\n",
    "        callbacks=[lgb.early_stopping(100, verbose=False), lgb.log_evaluation(100)],\n",
    "    )\n",
    "    val_pred = model.predict(X_val_fold)\n",
    "    val_pred_class = np.argmax(val_pred, axis=1)\n",
    "    accuracy = (val_pred_class == y_val_fold).mean()\n",
    "    cv_accuracies.append(accuracy)\n",
    "    y_val_relevance = np.zeros((len(y_val_fold), 3))\n",
    "    for i, class_id in enumerate(y_val_fold):\n",
    "        y_val_relevance[i, class_id] = class_id + 1\n",
    "    ndcg_val = ndcg_score(y_val_relevance, val_pred, k=20)\n",
    "    cv_scores.append(ndcg_val)\n",
    "    cv_models.append(model)\n",
    "    del train_data, val_data, X_train_fold, X_val_fold, y_train_fold, y_val_fold\n",
    "    gc.collect()\n",
    "\n",
    "for i, model in enumerate(cv_models):\n",
    "    model.save_model(str(MODEL_DIR / f\"model_fold_{i}.txt\"))\n",
    "\n",
    "with open(MODEL_DIR / \"features.json\", \"w\") as f:\n",
    "    json.dump(available_features, f)\n",
    "\n",
    "def expand_candidates_simple(df):\n",
    "    expanded = []\n",
    "    for _, row in df.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        book_list_str = row['book_id_list']\n",
    "        if isinstance(book_list_str, str):\n",
    "            try:\n",
    "                book_str = book_list_str.strip('[]')\n",
    "                book_ids = [int(b.strip()) for b in book_str.split(',') if b.strip()]\n",
    "            except:\n",
    "                book_ids = []\n",
    "        else:\n",
    "            book_ids = []\n",
    "        for book_id in book_ids:\n",
    "            expanded.append({'user_id': int(user_id), 'book_id': int(book_id)})\n",
    "    return pd.DataFrame(expanded)\n",
    "\n",
    "def generate_submission_with_hierarchy(candidates_features, models, train_pairs_set, available_features):\n",
    "    X_test = candidates_features[available_features].copy()\n",
    "    test_pred = np.zeros((len(X_test), 3))\n",
    "    for model in models:\n",
    "        test_pred += model.predict(X_test)\n",
    "    test_pred /= len(models)\n",
    "    candidates_features['prob_class_2'] = test_pred[:, 2]\n",
    "    candidates_features['prob_class_1'] = test_pred[:, 1]\n",
    "    candidates_features['prob_class_0'] = test_pred[:, 0]\n",
    "    candidates_features['predicted_class'] = np.argmax(test_pred, axis=1)\n",
    "    candidates_features['has_interaction'] = candidates_features.apply(lambda row: 1 if (row['user_id'], row['book_id']) in train_pairs_set else 0, axis=1)\n",
    "    candidates_features['hierarchical_score'] = (\n",
    "        candidates_features['predicted_class'] * 1000000 +\n",
    "        candidates_features['prob_class_2'] * 10000 +\n",
    "        candidates_features['prob_class_1'] * 1000 +\n",
    "        candidates_features['prob_class_0'] * 100 +\n",
    "        candidates_features['book_read_rate'] * 10 * candidates_features['predicted_class'].map({2: 1, 1: 0.5, 0: 0.1}) +\n",
    "        candidates_features['read_rate'] * 5 * candidates_features['predicted_class'].map({2: 1, 1: 0.8, 0: 0.1}) +\n",
    "        candidates_features['has_interaction'] * 50000\n",
    "    )\n",
    "    candidates_sorted = candidates_features.sort_values(['user_id', 'hierarchical_score'], ascending=[True, False]).drop_duplicates(subset=['user_id', 'book_id'], keep='first')\n",
    "    submission_rows = []\n",
    "    for user_id in candidates_sorted['user_id'].unique():\n",
    "        user_books = candidates_sorted[candidates_sorted['user_id'] == user_id]\n",
    "        seen = set()\n",
    "        top_books = []\n",
    "        for class_id in [2, 1, 0]:\n",
    "            class_books = user_books[user_books['predicted_class'] == class_id]\n",
    "            for book_id in class_books['book_id'].values:\n",
    "                if book_id not in seen:\n",
    "                    seen.add(book_id)\n",
    "                    top_books.append(book_id)\n",
    "                if len(top_books) >= 20:\n",
    "                    break\n",
    "            if len(top_books) >= 20:\n",
    "                break\n",
    "        submission_rows.append({'user_id': user_id, 'book_id_list': ','.join(map(str, top_books[:20]))})\n",
    "    return pd.DataFrame(submission_rows)\n",
    "\n",
    "candidates_expanded = expand_candidates_simple(candidates)\n",
    "candidates_features = candidates_expanded.merge(user_features, on='user_id', how='left')\n",
    "candidates_features = candidates_features.merge(book_features, on='book_id', how='left')\n",
    "candidates_features['read_rate_diff'] = candidates_features['read_rate'] - candidates_features['book_read_rate']\n",
    "candidates_features['interaction_ratio'] = candidates_features['total_interactions'] / (candidates_features['book_interactions'] + 1)\n",
    "candidates_features['compatibility_score'] = (candidates_features['read_rate'] * candidates_features['book_read_rate'] + candidates_features['diversity'] * candidates_features['book_diversity'])\n",
    "candidates_features['user_book_engagement_ratio'] = candidates_features['engagement'] / (candidates_features['book_popularity'] + 1e-10)\n",
    "candidates_features['user_book_diversity_product'] = candidates_features['diversity'] * candidates_features['book_diversity']\n",
    "candidates_features['is_likely_read'] = ((candidates_features['read_rate'] > 0.5) & (candidates_features['book_read_rate'] > 0.3)).astype(int)\n",
    "candidates_features['is_likely_plan'] = ((candidates_features['plan_rate'] > 0.5) & (candidates_features['book_read_rate'] < 0.3)).astype(int)\n",
    "candidates_features['has_interaction'] = candidates_features.apply(lambda row: 1 if (row['user_id'], row['book_id']) in train_pairs_set else 0, axis=1)\n",
    "\n",
    "for col in available_features:\n",
    "    if col not in candidates_features.columns:\n",
    "        candidates_features[col] = 0\n",
    "    elif candidates_features[col].isna().any():\n",
    "        candidates_features[col] = candidates_features[col].fillna(0)\n",
    "\n",
    "submission = generate_submission_with_hierarchy(candidates_features, cv_models, train_pairs_set, available_features)\n",
    "\n",
    "target_users_set = set(targets['user_id'])\n",
    "submission_users_set = set(submission['user_id'])\n",
    "missing_users = target_users_set - submission_users_set\n",
    "\n",
    "if missing_users:\n",
    "    missing_rows = [{'user_id': user_id, 'book_id_list': ''} for user_id in missing_users]\n",
    "    submission = pd.concat([submission, pd.DataFrame(missing_rows)], ignore_index=True)\n",
    "\n",
    "submission = submission.sort_values('user_id')\n",
    "\n",
    "for idx, row in submission.iterrows():\n",
    "    if pd.notna(row['book_id_list']) and row['book_id_list'] != '':\n",
    "        books = list(map(int, str(row['book_id_list']).split(',')))\n",
    "        if len(books) != len(set(books)):\n",
    "            unique_books = []\n",
    "            seen = set()\n",
    "            for book in books:\n",
    "                if book not in seen:\n",
    "                    seen.add(book)\n",
    "                    unique_books.append(book)\n",
    "            submission.at[idx, 'book_id_list'] = ','.join(map(str, unique_books[:20]))\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "final_path = SUBMISSION_DIR / f\"submission_gpu_enhanced_{timestamp}.csv\"\n",
    "submission[['user_id', 'book_id_list']].to_csv(final_path, index=False)\n",
    "\n",
    "print(f\"\\nРЕЗУЛЬТАТЫ:\")\n",
    "print(f\"Средняя точность: {np.mean(cv_accuracies):.4f}\")\n",
    "print(f\"Средний NDCG@20: {np.mean(cv_scores):.4f}\")\n",
    "print(f\"Submission сохранен: {final_path}\")\n",
    "\n",
    "del X, y, train_features, candidates_features\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nГотово!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
